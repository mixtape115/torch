{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import japanize_matplotlib\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch関連ライブラリのインポート\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元データ (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# 学習用データ準備\n",
    "\n",
    "# ライブラリのインポート\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# データ読み込み\n",
    "iris = load_iris()\n",
    "\n",
    "# 入力データと正解データ取得\n",
    "x_org, y_org = iris.data, iris.target\n",
    "\n",
    "# 結果確認\n",
    "print('元データ', x_org.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元データ (150, 2) (150,)\n"
     ]
    }
   ],
   "source": [
    "# データの絞り込み\n",
    "\n",
    "# 入力データに関しては、sepal(切片０)length(0) と petal（花弁）length(2) のみ抽出\n",
    "x_select = x_org[:,[0, 2]]\n",
    "\n",
    "# 結果確認\n",
    "print('元データ', x_select.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2) (75, 2) (75,) (75,)\n"
     ]
    }
   ],
   "source": [
    "# 訓練データ、検証データに分割（シャッフルも同時に実施）\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_select, y_org, train_size=75, test_size=75,\n",
    "    random_state=123)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを正解値ごとに分割\n",
    "\n",
    "x_t0 = x_train[y_train == 0]\n",
    "x_t1 = x_train[y_train == 1]\n",
    "x_t2 = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 散布図の表示\n",
    "\n",
    "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='k', s=50, label='0 (setosa)')\n",
    "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='b', s=50, label='1 (versicolour)')\n",
    "plt.scatter(x_t2[:,0], x_t2[:,1], marker='+', c='k', s=50, label='2 (virginica)')\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('petal_length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3 4.7]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_input: 2 n_output: 3\n"
     ]
    }
   ],
   "source": [
    "# 学習用パラメータ設定\n",
    "\n",
    "# 入力次元数\n",
    "n_input = x_train.shape[1]\n",
    "\n",
    "# 出力次元数\n",
    "# 分類先クラス数　今回は３\n",
    "n_output = len(list(set(y_train)))\n",
    "\n",
    "# 結果確認\n",
    "print(f'n_input: {n_input} n_output: {n_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "# 2入力3出力のロジスティック回帰モデル\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_output)\n",
    "        \n",
    "        # 初期値を全部１にする\n",
    "        self.l1.weight.data.fill_(1.0)\n",
    "        self.l1.bias.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.l1(x)\n",
    "        return x1\n",
    "    \n",
    "# インスタンスの生成\n",
    "net = Net(n_input, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1.weight', Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], requires_grad=True))\n",
      "('l1.bias', Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "#モデル内のパラメータの確認\n",
    "#l1.weightが行列に、l1.biasがベクトルになったいる\n",
    "\n",
    "for parameter in net.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 学習率\n",
    "lr = 0.01\n",
    "\n",
    "# 最適化関数\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力データx_trainと正解データy_train のテンソル変数化\n",
    "\n",
    "inputs = torch.tensor(x_train).float()\n",
    "labels = torch.tensor(y_train).long()\n",
    "\n",
    "inputs_test = torch.tensor(x_test).float()\n",
    "labels_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"213pt\" height=\"391pt\"\n",
       " viewBox=\"0.00 0.00 213.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-387 209,-387 209,4 -4,4\"/>\n",
       "<!-- 140617453940880 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140617453940880</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"129.5,-31 75.5,-31 75.5,0 129.5,0 129.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n",
       "</g>\n",
       "<!-- 140617453445192 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140617453445192</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"156,-86 49,-86 49,-67 156,-67 156,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">NllLossBackward</text>\n",
       "</g>\n",
       "<!-- 140617453445192&#45;&gt;140617453940880 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140617453445192&#45;&gt;140617453940880</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-66.9688C102.5,-60.1289 102.5,-50.5621 102.5,-41.5298\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-41.3678 102.5,-31.3678 99.0001,-41.3678 106.0001,-41.3678\"/>\n",
       "</g>\n",
       "<!-- 140617453445472 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140617453445472</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"165,-141 40,-141 40,-122 165,-122 165,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n",
       "</g>\n",
       "<!-- 140617453445472&#45;&gt;140617453445192 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140617453445472&#45;&gt;140617453445192</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-121.9197C102.5,-114.9083 102.5,-105.1442 102.5,-96.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-96.3408 102.5,-86.3408 99.0001,-96.3409 106.0001,-96.3408\"/>\n",
       "</g>\n",
       "<!-- 140617453445696 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140617453445696</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"150,-196 55,-196 55,-177 150,-177 150,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140617453445696&#45;&gt;140617453445472 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140617453445696&#45;&gt;140617453445472</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-176.9197C102.5,-169.9083 102.5,-160.1442 102.5,-151.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-151.3408 102.5,-141.3408 99.0001,-151.3409 106.0001,-151.3408\"/>\n",
       "</g>\n",
       "<!-- 140617453445640 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140617453445640</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140617453445640&#45;&gt;140617453445696 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140617453445640&#45;&gt;140617453445696</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M59.5577,-231.9197C66.9232,-224.1293 77.5017,-212.9405 86.3161,-203.6176\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.8691,-206.0119 93.1959,-196.3408 83.7825,-201.2028 88.8691,-206.0119\"/>\n",
       "</g>\n",
       "<!-- 140617459156600 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140617459156600</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-317 21,-317 21,-287 80,-287 80,-317\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">l1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 140617459156600&#45;&gt;140617453445640 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140617459156600&#45;&gt;140617453445640</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-286.7333C50.5,-279.0322 50.5,-269.5977 50.5,-261.3414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-261.0864 50.5,-251.0864 47.0001,-261.0864 54.0001,-261.0864\"/>\n",
       "</g>\n",
       "<!-- 140617453446536 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140617453446536</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"190,-251 119,-251 119,-232 190,-232 190,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140617453446536&#45;&gt;140617453445696 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140617453446536&#45;&gt;140617453445696</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.4423,-231.9197C138.0768,-224.1293 127.4983,-212.9405 118.6839,-203.6176\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.2175,-201.2028 111.8041,-196.3408 116.1309,-206.0119 121.2175,-201.2028\"/>\n",
       "</g>\n",
       "<!-- 140617453445248 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140617453445248</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205,-311.5 104,-311.5 104,-292.5 205,-292.5 205,-311.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140617453445248&#45;&gt;140617453446536 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140617453445248&#45;&gt;140617453446536</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M154.5,-292.2796C154.5,-284.0376 154.5,-271.9457 154.5,-261.629\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.0001,-261.3972 154.5,-251.3972 151.0001,-261.3973 158.0001,-261.3972\"/>\n",
       "</g>\n",
       "<!-- 140617459157968 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140617459157968</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"190,-383 119,-383 119,-353 190,-353 190,-383\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">l1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (3, 2)</text>\n",
       "</g>\n",
       "<!-- 140617459157968&#45;&gt;140617453445248 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140617459157968&#45;&gt;140617453445248</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M154.5,-352.6924C154.5,-343.5067 154.5,-331.7245 154.5,-321.8312\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.0001,-321.703 154.5,-311.7031 151.0001,-321.7031 158.0001,-321.703\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fe40d5ca128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 予測計算\n",
    "outputs = net(inputs)\n",
    "\n",
    "#損失計算\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "#損失の計算グラフ化\n",
    "g = make_dot(loss, params=dict(net.named_parameters()))\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([12.0000, 12.7000,  7.6000, 13.0000, 12.3000,  7.6000,  7.3000, 11.1000,\n",
      "        12.1000, 13.3000,  8.0000,  7.0000, 10.3000,  7.6000, 11.7000, 13.3000,\n",
      "         7.4000, 13.5000,  8.2000,  8.4000, 12.7000,  6.6000,  7.9000, 12.2000,\n",
      "        14.6000, 12.0000, 10.2000, 10.5000,  7.1000,  7.3000, 12.6000, 12.7000,\n",
      "         7.4000,  7.7000, 10.8000, 11.5000, 11.5000, 14.0000, 12.8000, 10.8000,\n",
      "        10.8000, 15.2000,  7.5000,  7.8000, 11.1000, 13.6000, 12.9000, 14.2000,\n",
      "        12.7000,  7.6000, 10.9000,  7.0000, 10.9000, 11.2000,  7.4000, 11.7000,\n",
      "        13.3000, 11.5000, 13.4000, 12.7000,  7.7000, 11.8000,  7.0000, 12.6000,\n",
      "        11.7000, 10.9000,  9.2000, 12.2000, 10.4000, 12.1000,  7.5000,  9.1000,\n",
      "        11.1000, 12.0000, 14.3000], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "#torch.max関数呼び出し\n",
    "#２つめの引数は軸を意味している。１だと行ごとの計算\n",
    "\n",
    "print(torch.max(outputs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#予測データのリストを取得\n",
    "torch.max(outputs, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率\n",
    "lr = 0.01\n",
    "\n",
    "# 初期化\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 損失関数： 交差エントロピー関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化関数: 勾配降下法\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 繰り返し回数\n",
    "num_epochs = 10000\n",
    "\n",
    "# 評価結果記録用\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], loss: 1.09861 acc: 0.30667 val_loss: 1.09263, val_acc: 0.26667\n",
      "Epoch [10/10000], loss: 1.03580 acc: 0.40000 val_loss: 1.06403, val_acc: 0.26667\n",
      "Epoch [20/10000], loss: 1.00477 acc: 0.40000 val_loss: 1.03347, val_acc: 0.26667\n",
      "Epoch [30/10000], loss: 0.97672 acc: 0.40000 val_loss: 1.00264, val_acc: 0.26667\n",
      "Epoch [40/10000], loss: 0.95057 acc: 0.41333 val_loss: 0.97351, val_acc: 0.26667\n",
      "Epoch [50/10000], loss: 0.92616 acc: 0.48000 val_loss: 0.94631, val_acc: 0.38667\n",
      "Epoch [60/10000], loss: 0.90338 acc: 0.69333 val_loss: 0.92098, val_acc: 0.56000\n",
      "Epoch [70/10000], loss: 0.88212 acc: 0.70667 val_loss: 0.89740, val_acc: 0.60000\n",
      "Epoch [80/10000], loss: 0.86227 acc: 0.70667 val_loss: 0.87545, val_acc: 0.61333\n",
      "Epoch [90/10000], loss: 0.84373 acc: 0.70667 val_loss: 0.85500, val_acc: 0.62667\n",
      "Epoch [100/10000], loss: 0.82640 acc: 0.70667 val_loss: 0.83594, val_acc: 0.62667\n",
      "Epoch [110/10000], loss: 0.81019 acc: 0.72000 val_loss: 0.81815, val_acc: 0.62667\n",
      "Epoch [120/10000], loss: 0.79500 acc: 0.72000 val_loss: 0.80153, val_acc: 0.62667\n",
      "Epoch [130/10000], loss: 0.78077 acc: 0.73333 val_loss: 0.78599, val_acc: 0.62667\n",
      "Epoch [140/10000], loss: 0.76741 acc: 0.74667 val_loss: 0.77142, val_acc: 0.64000\n",
      "Epoch [150/10000], loss: 0.75485 acc: 0.74667 val_loss: 0.75777, val_acc: 0.65333\n",
      "Epoch [160/10000], loss: 0.74303 acc: 0.74667 val_loss: 0.74494, val_acc: 0.68000\n",
      "Epoch [170/10000], loss: 0.73189 acc: 0.76000 val_loss: 0.73288, val_acc: 0.70667\n",
      "Epoch [180/10000], loss: 0.72138 acc: 0.77333 val_loss: 0.72151, val_acc: 0.76000\n",
      "Epoch [190/10000], loss: 0.71145 acc: 0.82667 val_loss: 0.71079, val_acc: 0.78667\n",
      "Epoch [200/10000], loss: 0.70205 acc: 0.82667 val_loss: 0.70067, val_acc: 0.78667\n",
      "Epoch [210/10000], loss: 0.69315 acc: 0.84000 val_loss: 0.69109, val_acc: 0.80000\n",
      "Epoch [220/10000], loss: 0.68470 acc: 0.84000 val_loss: 0.68202, val_acc: 0.80000\n",
      "Epoch [230/10000], loss: 0.67667 acc: 0.86667 val_loss: 0.67341, val_acc: 0.81333\n",
      "Epoch [240/10000], loss: 0.66904 acc: 0.86667 val_loss: 0.66524, val_acc: 0.81333\n",
      "Epoch [250/10000], loss: 0.66176 acc: 0.86667 val_loss: 0.65746, val_acc: 0.82667\n",
      "Epoch [260/10000], loss: 0.65483 acc: 0.85333 val_loss: 0.65005, val_acc: 0.82667\n",
      "Epoch [270/10000], loss: 0.64820 acc: 0.85333 val_loss: 0.64299, val_acc: 0.82667\n",
      "Epoch [280/10000], loss: 0.64187 acc: 0.85333 val_loss: 0.63625, val_acc: 0.82667\n",
      "Epoch [290/10000], loss: 0.63581 acc: 0.86667 val_loss: 0.62980, val_acc: 0.82667\n",
      "Epoch [300/10000], loss: 0.63000 acc: 0.88000 val_loss: 0.62363, val_acc: 0.82667\n",
      "Epoch [310/10000], loss: 0.62443 acc: 0.89333 val_loss: 0.61772, val_acc: 0.82667\n",
      "Epoch [320/10000], loss: 0.61909 acc: 0.89333 val_loss: 0.61205, val_acc: 0.82667\n",
      "Epoch [330/10000], loss: 0.61394 acc: 0.89333 val_loss: 0.60661, val_acc: 0.82667\n",
      "Epoch [340/10000], loss: 0.60900 acc: 0.89333 val_loss: 0.60138, val_acc: 0.84000\n",
      "Epoch [350/10000], loss: 0.60423 acc: 0.89333 val_loss: 0.59635, val_acc: 0.84000\n",
      "Epoch [360/10000], loss: 0.59964 acc: 0.90667 val_loss: 0.59150, val_acc: 0.85333\n",
      "Epoch [370/10000], loss: 0.59521 acc: 0.92000 val_loss: 0.58683, val_acc: 0.86667\n",
      "Epoch [380/10000], loss: 0.59093 acc: 0.92000 val_loss: 0.58232, val_acc: 0.86667\n",
      "Epoch [390/10000], loss: 0.58679 acc: 0.92000 val_loss: 0.57797, val_acc: 0.86667\n",
      "Epoch [400/10000], loss: 0.58279 acc: 0.92000 val_loss: 0.57377, val_acc: 0.86667\n",
      "Epoch [410/10000], loss: 0.57891 acc: 0.92000 val_loss: 0.56970, val_acc: 0.86667\n",
      "Epoch [420/10000], loss: 0.57516 acc: 0.92000 val_loss: 0.56576, val_acc: 0.86667\n",
      "Epoch [430/10000], loss: 0.57152 acc: 0.90667 val_loss: 0.56195, val_acc: 0.86667\n",
      "Epoch [440/10000], loss: 0.56799 acc: 0.90667 val_loss: 0.55825, val_acc: 0.86667\n",
      "Epoch [450/10000], loss: 0.56456 acc: 0.90667 val_loss: 0.55466, val_acc: 0.86667\n",
      "Epoch [460/10000], loss: 0.56123 acc: 0.90667 val_loss: 0.55118, val_acc: 0.86667\n",
      "Epoch [470/10000], loss: 0.55799 acc: 0.90667 val_loss: 0.54779, val_acc: 0.88000\n",
      "Epoch [480/10000], loss: 0.55484 acc: 0.90667 val_loss: 0.54451, val_acc: 0.88000\n",
      "Epoch [490/10000], loss: 0.55177 acc: 0.90667 val_loss: 0.54131, val_acc: 0.88000\n",
      "Epoch [500/10000], loss: 0.54878 acc: 0.90667 val_loss: 0.53819, val_acc: 0.88000\n",
      "Epoch [510/10000], loss: 0.54587 acc: 0.90667 val_loss: 0.53516, val_acc: 0.88000\n",
      "Epoch [520/10000], loss: 0.54303 acc: 0.90667 val_loss: 0.53221, val_acc: 0.88000\n",
      "Epoch [530/10000], loss: 0.54026 acc: 0.90667 val_loss: 0.52933, val_acc: 0.88000\n",
      "Epoch [540/10000], loss: 0.53755 acc: 0.90667 val_loss: 0.52652, val_acc: 0.88000\n",
      "Epoch [550/10000], loss: 0.53491 acc: 0.90667 val_loss: 0.52377, val_acc: 0.88000\n",
      "Epoch [560/10000], loss: 0.53233 acc: 0.90667 val_loss: 0.52110, val_acc: 0.88000\n",
      "Epoch [570/10000], loss: 0.52981 acc: 0.90667 val_loss: 0.51848, val_acc: 0.88000\n",
      "Epoch [580/10000], loss: 0.52734 acc: 0.90667 val_loss: 0.51592, val_acc: 0.88000\n",
      "Epoch [590/10000], loss: 0.52493 acc: 0.90667 val_loss: 0.51342, val_acc: 0.88000\n",
      "Epoch [600/10000], loss: 0.52256 acc: 0.90667 val_loss: 0.51098, val_acc: 0.88000\n",
      "Epoch [610/10000], loss: 0.52025 acc: 0.90667 val_loss: 0.50859, val_acc: 0.88000\n",
      "Epoch [620/10000], loss: 0.51798 acc: 0.90667 val_loss: 0.50624, val_acc: 0.88000\n",
      "Epoch [630/10000], loss: 0.51576 acc: 0.90667 val_loss: 0.50395, val_acc: 0.88000\n",
      "Epoch [640/10000], loss: 0.51358 acc: 0.90667 val_loss: 0.50170, val_acc: 0.88000\n",
      "Epoch [650/10000], loss: 0.51144 acc: 0.90667 val_loss: 0.49949, val_acc: 0.88000\n",
      "Epoch [660/10000], loss: 0.50934 acc: 0.90667 val_loss: 0.49733, val_acc: 0.89333\n",
      "Epoch [670/10000], loss: 0.50728 acc: 0.90667 val_loss: 0.49521, val_acc: 0.90667\n",
      "Epoch [680/10000], loss: 0.50526 acc: 0.90667 val_loss: 0.49313, val_acc: 0.90667\n",
      "Epoch [690/10000], loss: 0.50328 acc: 0.90667 val_loss: 0.49109, val_acc: 0.90667\n",
      "Epoch [700/10000], loss: 0.50133 acc: 0.90667 val_loss: 0.48908, val_acc: 0.90667\n",
      "Epoch [710/10000], loss: 0.49941 acc: 0.90667 val_loss: 0.48711, val_acc: 0.90667\n",
      "Epoch [720/10000], loss: 0.49752 acc: 0.90667 val_loss: 0.48517, val_acc: 0.90667\n",
      "Epoch [730/10000], loss: 0.49567 acc: 0.90667 val_loss: 0.48327, val_acc: 0.90667\n",
      "Epoch [740/10000], loss: 0.49385 acc: 0.90667 val_loss: 0.48140, val_acc: 0.90667\n",
      "Epoch [750/10000], loss: 0.49205 acc: 0.90667 val_loss: 0.47956, val_acc: 0.90667\n",
      "Epoch [760/10000], loss: 0.49029 acc: 0.90667 val_loss: 0.47775, val_acc: 0.90667\n",
      "Epoch [770/10000], loss: 0.48855 acc: 0.90667 val_loss: 0.47597, val_acc: 0.90667\n",
      "Epoch [780/10000], loss: 0.48684 acc: 0.90667 val_loss: 0.47422, val_acc: 0.90667\n",
      "Epoch [790/10000], loss: 0.48515 acc: 0.89333 val_loss: 0.47249, val_acc: 0.92000\n",
      "Epoch [800/10000], loss: 0.48349 acc: 0.89333 val_loss: 0.47079, val_acc: 0.92000\n",
      "Epoch [810/10000], loss: 0.48186 acc: 0.89333 val_loss: 0.46912, val_acc: 0.92000\n",
      "Epoch [820/10000], loss: 0.48024 acc: 0.89333 val_loss: 0.46747, val_acc: 0.92000\n",
      "Epoch [830/10000], loss: 0.47865 acc: 0.89333 val_loss: 0.46585, val_acc: 0.92000\n",
      "Epoch [840/10000], loss: 0.47709 acc: 0.89333 val_loss: 0.46425, val_acc: 0.92000\n",
      "Epoch [850/10000], loss: 0.47554 acc: 0.89333 val_loss: 0.46267, val_acc: 0.92000\n",
      "Epoch [860/10000], loss: 0.47402 acc: 0.89333 val_loss: 0.46111, val_acc: 0.92000\n",
      "Epoch [870/10000], loss: 0.47251 acc: 0.89333 val_loss: 0.45958, val_acc: 0.92000\n",
      "Epoch [880/10000], loss: 0.47103 acc: 0.89333 val_loss: 0.45806, val_acc: 0.92000\n",
      "Epoch [890/10000], loss: 0.46956 acc: 0.89333 val_loss: 0.45657, val_acc: 0.92000\n",
      "Epoch [900/10000], loss: 0.46811 acc: 0.89333 val_loss: 0.45509, val_acc: 0.92000\n",
      "Epoch [910/10000], loss: 0.46668 acc: 0.89333 val_loss: 0.45364, val_acc: 0.92000\n",
      "Epoch [920/10000], loss: 0.46527 acc: 0.89333 val_loss: 0.45220, val_acc: 0.92000\n",
      "Epoch [930/10000], loss: 0.46388 acc: 0.89333 val_loss: 0.45078, val_acc: 0.92000\n",
      "Epoch [940/10000], loss: 0.46250 acc: 0.89333 val_loss: 0.44938, val_acc: 0.92000\n",
      "Epoch [950/10000], loss: 0.46114 acc: 0.89333 val_loss: 0.44800, val_acc: 0.92000\n",
      "Epoch [960/10000], loss: 0.45980 acc: 0.89333 val_loss: 0.44663, val_acc: 0.92000\n",
      "Epoch [970/10000], loss: 0.45847 acc: 0.89333 val_loss: 0.44528, val_acc: 0.92000\n",
      "Epoch [980/10000], loss: 0.45716 acc: 0.89333 val_loss: 0.44395, val_acc: 0.92000\n",
      "Epoch [990/10000], loss: 0.45586 acc: 0.89333 val_loss: 0.44263, val_acc: 0.92000\n",
      "Epoch [1000/10000], loss: 0.45458 acc: 0.89333 val_loss: 0.44133, val_acc: 0.92000\n",
      "Epoch [1010/10000], loss: 0.45331 acc: 0.89333 val_loss: 0.44004, val_acc: 0.92000\n",
      "Epoch [1020/10000], loss: 0.45205 acc: 0.89333 val_loss: 0.43877, val_acc: 0.92000\n",
      "Epoch [1030/10000], loss: 0.45081 acc: 0.89333 val_loss: 0.43751, val_acc: 0.92000\n",
      "Epoch [1040/10000], loss: 0.44958 acc: 0.89333 val_loss: 0.43626, val_acc: 0.92000\n",
      "Epoch [1050/10000], loss: 0.44836 acc: 0.89333 val_loss: 0.43503, val_acc: 0.92000\n",
      "Epoch [1060/10000], loss: 0.44716 acc: 0.89333 val_loss: 0.43381, val_acc: 0.92000\n",
      "Epoch [1070/10000], loss: 0.44597 acc: 0.89333 val_loss: 0.43260, val_acc: 0.92000\n",
      "Epoch [1080/10000], loss: 0.44479 acc: 0.89333 val_loss: 0.43141, val_acc: 0.92000\n",
      "Epoch [1090/10000], loss: 0.44363 acc: 0.89333 val_loss: 0.43023, val_acc: 0.92000\n",
      "Epoch [1100/10000], loss: 0.44247 acc: 0.89333 val_loss: 0.42906, val_acc: 0.92000\n",
      "Epoch [1110/10000], loss: 0.44133 acc: 0.89333 val_loss: 0.42790, val_acc: 0.92000\n",
      "Epoch [1120/10000], loss: 0.44020 acc: 0.89333 val_loss: 0.42676, val_acc: 0.92000\n",
      "Epoch [1130/10000], loss: 0.43908 acc: 0.89333 val_loss: 0.42562, val_acc: 0.92000\n",
      "Epoch [1140/10000], loss: 0.43797 acc: 0.89333 val_loss: 0.42450, val_acc: 0.92000\n",
      "Epoch [1150/10000], loss: 0.43687 acc: 0.89333 val_loss: 0.42339, val_acc: 0.92000\n",
      "Epoch [1160/10000], loss: 0.43578 acc: 0.89333 val_loss: 0.42229, val_acc: 0.92000\n",
      "Epoch [1170/10000], loss: 0.43470 acc: 0.89333 val_loss: 0.42120, val_acc: 0.92000\n",
      "Epoch [1180/10000], loss: 0.43363 acc: 0.89333 val_loss: 0.42012, val_acc: 0.92000\n",
      "Epoch [1190/10000], loss: 0.43257 acc: 0.89333 val_loss: 0.41905, val_acc: 0.92000\n",
      "Epoch [1200/10000], loss: 0.43152 acc: 0.89333 val_loss: 0.41799, val_acc: 0.92000\n",
      "Epoch [1210/10000], loss: 0.43048 acc: 0.89333 val_loss: 0.41694, val_acc: 0.92000\n",
      "Epoch [1220/10000], loss: 0.42945 acc: 0.89333 val_loss: 0.41590, val_acc: 0.92000\n",
      "Epoch [1230/10000], loss: 0.42843 acc: 0.89333 val_loss: 0.41487, val_acc: 0.92000\n",
      "Epoch [1240/10000], loss: 0.42742 acc: 0.89333 val_loss: 0.41384, val_acc: 0.92000\n",
      "Epoch [1250/10000], loss: 0.42641 acc: 0.89333 val_loss: 0.41283, val_acc: 0.92000\n",
      "Epoch [1260/10000], loss: 0.42542 acc: 0.89333 val_loss: 0.41182, val_acc: 0.92000\n",
      "Epoch [1270/10000], loss: 0.42443 acc: 0.89333 val_loss: 0.41083, val_acc: 0.92000\n",
      "Epoch [1280/10000], loss: 0.42345 acc: 0.89333 val_loss: 0.40984, val_acc: 0.92000\n",
      "Epoch [1290/10000], loss: 0.42248 acc: 0.89333 val_loss: 0.40886, val_acc: 0.92000\n",
      "Epoch [1300/10000], loss: 0.42152 acc: 0.89333 val_loss: 0.40789, val_acc: 0.92000\n",
      "Epoch [1310/10000], loss: 0.42056 acc: 0.89333 val_loss: 0.40693, val_acc: 0.92000\n",
      "Epoch [1320/10000], loss: 0.41962 acc: 0.89333 val_loss: 0.40598, val_acc: 0.92000\n",
      "Epoch [1330/10000], loss: 0.41868 acc: 0.89333 val_loss: 0.40503, val_acc: 0.93333\n",
      "Epoch [1340/10000], loss: 0.41775 acc: 0.89333 val_loss: 0.40409, val_acc: 0.93333\n",
      "Epoch [1350/10000], loss: 0.41682 acc: 0.89333 val_loss: 0.40316, val_acc: 0.93333\n",
      "Epoch [1360/10000], loss: 0.41590 acc: 0.89333 val_loss: 0.40224, val_acc: 0.93333\n",
      "Epoch [1370/10000], loss: 0.41499 acc: 0.89333 val_loss: 0.40132, val_acc: 0.93333\n",
      "Epoch [1380/10000], loss: 0.41409 acc: 0.89333 val_loss: 0.40041, val_acc: 0.93333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1390/10000], loss: 0.41320 acc: 0.89333 val_loss: 0.39951, val_acc: 0.93333\n",
      "Epoch [1400/10000], loss: 0.41231 acc: 0.89333 val_loss: 0.39861, val_acc: 0.93333\n",
      "Epoch [1410/10000], loss: 0.41143 acc: 0.89333 val_loss: 0.39773, val_acc: 0.93333\n",
      "Epoch [1420/10000], loss: 0.41055 acc: 0.89333 val_loss: 0.39685, val_acc: 0.93333\n",
      "Epoch [1430/10000], loss: 0.40968 acc: 0.89333 val_loss: 0.39597, val_acc: 0.93333\n",
      "Epoch [1440/10000], loss: 0.40882 acc: 0.89333 val_loss: 0.39510, val_acc: 0.93333\n",
      "Epoch [1450/10000], loss: 0.40796 acc: 0.89333 val_loss: 0.39424, val_acc: 0.93333\n",
      "Epoch [1460/10000], loss: 0.40711 acc: 0.89333 val_loss: 0.39339, val_acc: 0.93333\n",
      "Epoch [1470/10000], loss: 0.40627 acc: 0.89333 val_loss: 0.39254, val_acc: 0.93333\n",
      "Epoch [1480/10000], loss: 0.40543 acc: 0.90667 val_loss: 0.39170, val_acc: 0.93333\n",
      "Epoch [1490/10000], loss: 0.40460 acc: 0.90667 val_loss: 0.39086, val_acc: 0.93333\n",
      "Epoch [1500/10000], loss: 0.40378 acc: 0.90667 val_loss: 0.39003, val_acc: 0.93333\n",
      "Epoch [1510/10000], loss: 0.40296 acc: 0.90667 val_loss: 0.38921, val_acc: 0.93333\n",
      "Epoch [1520/10000], loss: 0.40214 acc: 0.90667 val_loss: 0.38839, val_acc: 0.93333\n",
      "Epoch [1530/10000], loss: 0.40134 acc: 0.90667 val_loss: 0.38758, val_acc: 0.93333\n",
      "Epoch [1540/10000], loss: 0.40053 acc: 0.90667 val_loss: 0.38677, val_acc: 0.93333\n",
      "Epoch [1550/10000], loss: 0.39974 acc: 0.90667 val_loss: 0.38597, val_acc: 0.93333\n",
      "Epoch [1560/10000], loss: 0.39894 acc: 0.90667 val_loss: 0.38517, val_acc: 0.94667\n",
      "Epoch [1570/10000], loss: 0.39816 acc: 0.90667 val_loss: 0.38438, val_acc: 0.94667\n",
      "Epoch [1580/10000], loss: 0.39738 acc: 0.90667 val_loss: 0.38360, val_acc: 0.94667\n",
      "Epoch [1590/10000], loss: 0.39660 acc: 0.90667 val_loss: 0.38282, val_acc: 0.94667\n",
      "Epoch [1600/10000], loss: 0.39583 acc: 0.90667 val_loss: 0.38204, val_acc: 0.94667\n",
      "Epoch [1610/10000], loss: 0.39507 acc: 0.90667 val_loss: 0.38128, val_acc: 0.94667\n",
      "Epoch [1620/10000], loss: 0.39431 acc: 0.90667 val_loss: 0.38051, val_acc: 0.94667\n",
      "Epoch [1630/10000], loss: 0.39355 acc: 0.90667 val_loss: 0.37975, val_acc: 0.94667\n",
      "Epoch [1640/10000], loss: 0.39280 acc: 0.90667 val_loss: 0.37900, val_acc: 0.94667\n",
      "Epoch [1650/10000], loss: 0.39206 acc: 0.90667 val_loss: 0.37825, val_acc: 0.94667\n",
      "Epoch [1660/10000], loss: 0.39132 acc: 0.90667 val_loss: 0.37751, val_acc: 0.94667\n",
      "Epoch [1670/10000], loss: 0.39058 acc: 0.90667 val_loss: 0.37677, val_acc: 0.94667\n",
      "Epoch [1680/10000], loss: 0.38985 acc: 0.90667 val_loss: 0.37604, val_acc: 0.94667\n",
      "Epoch [1690/10000], loss: 0.38913 acc: 0.90667 val_loss: 0.37531, val_acc: 0.94667\n",
      "Epoch [1700/10000], loss: 0.38841 acc: 0.90667 val_loss: 0.37458, val_acc: 0.94667\n",
      "Epoch [1710/10000], loss: 0.38769 acc: 0.90667 val_loss: 0.37386, val_acc: 0.94667\n",
      "Epoch [1720/10000], loss: 0.38698 acc: 0.90667 val_loss: 0.37315, val_acc: 0.94667\n",
      "Epoch [1730/10000], loss: 0.38627 acc: 0.90667 val_loss: 0.37244, val_acc: 0.94667\n",
      "Epoch [1740/10000], loss: 0.38557 acc: 0.90667 val_loss: 0.37173, val_acc: 0.94667\n",
      "Epoch [1750/10000], loss: 0.38487 acc: 0.90667 val_loss: 0.37103, val_acc: 0.94667\n",
      "Epoch [1760/10000], loss: 0.38417 acc: 0.90667 val_loss: 0.37033, val_acc: 0.94667\n",
      "Epoch [1770/10000], loss: 0.38348 acc: 0.90667 val_loss: 0.36964, val_acc: 0.94667\n",
      "Epoch [1780/10000], loss: 0.38280 acc: 0.90667 val_loss: 0.36895, val_acc: 0.94667\n",
      "Epoch [1790/10000], loss: 0.38212 acc: 0.90667 val_loss: 0.36826, val_acc: 0.94667\n",
      "Epoch [1800/10000], loss: 0.38144 acc: 0.90667 val_loss: 0.36758, val_acc: 0.94667\n",
      "Epoch [1810/10000], loss: 0.38076 acc: 0.90667 val_loss: 0.36690, val_acc: 0.94667\n",
      "Epoch [1820/10000], loss: 0.38009 acc: 0.90667 val_loss: 0.36623, val_acc: 0.94667\n",
      "Epoch [1830/10000], loss: 0.37943 acc: 0.90667 val_loss: 0.36556, val_acc: 0.94667\n",
      "Epoch [1840/10000], loss: 0.37877 acc: 0.90667 val_loss: 0.36490, val_acc: 0.94667\n",
      "Epoch [1850/10000], loss: 0.37811 acc: 0.90667 val_loss: 0.36424, val_acc: 0.94667\n",
      "Epoch [1860/10000], loss: 0.37746 acc: 0.90667 val_loss: 0.36358, val_acc: 0.94667\n",
      "Epoch [1870/10000], loss: 0.37681 acc: 0.90667 val_loss: 0.36293, val_acc: 0.94667\n",
      "Epoch [1880/10000], loss: 0.37616 acc: 0.90667 val_loss: 0.36228, val_acc: 0.94667\n",
      "Epoch [1890/10000], loss: 0.37552 acc: 0.90667 val_loss: 0.36163, val_acc: 0.94667\n",
      "Epoch [1900/10000], loss: 0.37488 acc: 0.90667 val_loss: 0.36099, val_acc: 0.94667\n",
      "Epoch [1910/10000], loss: 0.37424 acc: 0.90667 val_loss: 0.36035, val_acc: 0.94667\n",
      "Epoch [1920/10000], loss: 0.37361 acc: 0.90667 val_loss: 0.35972, val_acc: 0.94667\n",
      "Epoch [1930/10000], loss: 0.37298 acc: 0.90667 val_loss: 0.35909, val_acc: 0.94667\n",
      "Epoch [1940/10000], loss: 0.37236 acc: 0.90667 val_loss: 0.35846, val_acc: 0.94667\n",
      "Epoch [1950/10000], loss: 0.37174 acc: 0.90667 val_loss: 0.35784, val_acc: 0.94667\n",
      "Epoch [1960/10000], loss: 0.37112 acc: 0.90667 val_loss: 0.35722, val_acc: 0.94667\n",
      "Epoch [1970/10000], loss: 0.37051 acc: 0.90667 val_loss: 0.35660, val_acc: 0.94667\n",
      "Epoch [1980/10000], loss: 0.36990 acc: 0.90667 val_loss: 0.35599, val_acc: 0.94667\n",
      "Epoch [1990/10000], loss: 0.36929 acc: 0.90667 val_loss: 0.35538, val_acc: 0.94667\n",
      "Epoch [2000/10000], loss: 0.36869 acc: 0.90667 val_loss: 0.35477, val_acc: 0.94667\n",
      "Epoch [2010/10000], loss: 0.36809 acc: 0.90667 val_loss: 0.35417, val_acc: 0.94667\n",
      "Epoch [2020/10000], loss: 0.36749 acc: 0.90667 val_loss: 0.35357, val_acc: 0.94667\n",
      "Epoch [2030/10000], loss: 0.36690 acc: 0.90667 val_loss: 0.35298, val_acc: 0.94667\n",
      "Epoch [2040/10000], loss: 0.36631 acc: 0.90667 val_loss: 0.35238, val_acc: 0.94667\n",
      "Epoch [2050/10000], loss: 0.36572 acc: 0.90667 val_loss: 0.35179, val_acc: 0.94667\n",
      "Epoch [2060/10000], loss: 0.36514 acc: 0.90667 val_loss: 0.35121, val_acc: 0.94667\n",
      "Epoch [2070/10000], loss: 0.36455 acc: 0.90667 val_loss: 0.35062, val_acc: 0.94667\n",
      "Epoch [2080/10000], loss: 0.36398 acc: 0.90667 val_loss: 0.35004, val_acc: 0.94667\n",
      "Epoch [2090/10000], loss: 0.36340 acc: 0.90667 val_loss: 0.34947, val_acc: 0.94667\n",
      "Epoch [2100/10000], loss: 0.36283 acc: 0.90667 val_loss: 0.34889, val_acc: 0.94667\n",
      "Epoch [2110/10000], loss: 0.36226 acc: 0.90667 val_loss: 0.34832, val_acc: 0.94667\n",
      "Epoch [2120/10000], loss: 0.36170 acc: 0.90667 val_loss: 0.34775, val_acc: 0.94667\n",
      "Epoch [2130/10000], loss: 0.36114 acc: 0.90667 val_loss: 0.34719, val_acc: 0.94667\n",
      "Epoch [2140/10000], loss: 0.36058 acc: 0.90667 val_loss: 0.34663, val_acc: 0.94667\n",
      "Epoch [2150/10000], loss: 0.36002 acc: 0.90667 val_loss: 0.34607, val_acc: 0.94667\n",
      "Epoch [2160/10000], loss: 0.35947 acc: 0.90667 val_loss: 0.34551, val_acc: 0.94667\n",
      "Epoch [2170/10000], loss: 0.35892 acc: 0.90667 val_loss: 0.34496, val_acc: 0.94667\n",
      "Epoch [2180/10000], loss: 0.35837 acc: 0.90667 val_loss: 0.34441, val_acc: 0.94667\n",
      "Epoch [2190/10000], loss: 0.35782 acc: 0.90667 val_loss: 0.34386, val_acc: 0.94667\n",
      "Epoch [2200/10000], loss: 0.35728 acc: 0.90667 val_loss: 0.34331, val_acc: 0.94667\n",
      "Epoch [2210/10000], loss: 0.35674 acc: 0.90667 val_loss: 0.34277, val_acc: 0.94667\n",
      "Epoch [2220/10000], loss: 0.35621 acc: 0.90667 val_loss: 0.34223, val_acc: 0.94667\n",
      "Epoch [2230/10000], loss: 0.35567 acc: 0.90667 val_loss: 0.34170, val_acc: 0.94667\n",
      "Epoch [2240/10000], loss: 0.35514 acc: 0.90667 val_loss: 0.34116, val_acc: 0.94667\n",
      "Epoch [2250/10000], loss: 0.35461 acc: 0.90667 val_loss: 0.34063, val_acc: 0.94667\n",
      "Epoch [2260/10000], loss: 0.35409 acc: 0.90667 val_loss: 0.34010, val_acc: 0.94667\n",
      "Epoch [2270/10000], loss: 0.35356 acc: 0.90667 val_loss: 0.33958, val_acc: 0.94667\n",
      "Epoch [2280/10000], loss: 0.35304 acc: 0.90667 val_loss: 0.33905, val_acc: 0.94667\n",
      "Epoch [2290/10000], loss: 0.35253 acc: 0.90667 val_loss: 0.33853, val_acc: 0.94667\n",
      "Epoch [2300/10000], loss: 0.35201 acc: 0.90667 val_loss: 0.33802, val_acc: 0.94667\n",
      "Epoch [2310/10000], loss: 0.35150 acc: 0.90667 val_loss: 0.33750, val_acc: 0.94667\n",
      "Epoch [2320/10000], loss: 0.35099 acc: 0.90667 val_loss: 0.33699, val_acc: 0.94667\n",
      "Epoch [2330/10000], loss: 0.35048 acc: 0.90667 val_loss: 0.33648, val_acc: 0.94667\n",
      "Epoch [2340/10000], loss: 0.34998 acc: 0.90667 val_loss: 0.33597, val_acc: 0.94667\n",
      "Epoch [2350/10000], loss: 0.34947 acc: 0.90667 val_loss: 0.33546, val_acc: 0.94667\n",
      "Epoch [2360/10000], loss: 0.34897 acc: 0.90667 val_loss: 0.33496, val_acc: 0.94667\n",
      "Epoch [2370/10000], loss: 0.34848 acc: 0.90667 val_loss: 0.33446, val_acc: 0.94667\n",
      "Epoch [2380/10000], loss: 0.34798 acc: 0.90667 val_loss: 0.33396, val_acc: 0.94667\n",
      "Epoch [2390/10000], loss: 0.34749 acc: 0.90667 val_loss: 0.33347, val_acc: 0.94667\n",
      "Epoch [2400/10000], loss: 0.34700 acc: 0.90667 val_loss: 0.33297, val_acc: 0.94667\n",
      "Epoch [2410/10000], loss: 0.34651 acc: 0.90667 val_loss: 0.33248, val_acc: 0.94667\n",
      "Epoch [2420/10000], loss: 0.34602 acc: 0.90667 val_loss: 0.33199, val_acc: 0.94667\n",
      "Epoch [2430/10000], loss: 0.34554 acc: 0.90667 val_loss: 0.33151, val_acc: 0.94667\n",
      "Epoch [2440/10000], loss: 0.34506 acc: 0.90667 val_loss: 0.33102, val_acc: 0.94667\n",
      "Epoch [2450/10000], loss: 0.34458 acc: 0.90667 val_loss: 0.33054, val_acc: 0.94667\n",
      "Epoch [2460/10000], loss: 0.34411 acc: 0.90667 val_loss: 0.33006, val_acc: 0.94667\n",
      "Epoch [2470/10000], loss: 0.34363 acc: 0.90667 val_loss: 0.32959, val_acc: 0.94667\n",
      "Epoch [2480/10000], loss: 0.34316 acc: 0.90667 val_loss: 0.32911, val_acc: 0.94667\n",
      "Epoch [2490/10000], loss: 0.34269 acc: 0.90667 val_loss: 0.32864, val_acc: 0.94667\n",
      "Epoch [2500/10000], loss: 0.34222 acc: 0.90667 val_loss: 0.32817, val_acc: 0.94667\n",
      "Epoch [2510/10000], loss: 0.34176 acc: 0.90667 val_loss: 0.32770, val_acc: 0.94667\n",
      "Epoch [2520/10000], loss: 0.34130 acc: 0.90667 val_loss: 0.32723, val_acc: 0.94667\n",
      "Epoch [2530/10000], loss: 0.34083 acc: 0.90667 val_loss: 0.32677, val_acc: 0.94667\n",
      "Epoch [2540/10000], loss: 0.34038 acc: 0.90667 val_loss: 0.32631, val_acc: 0.94667\n",
      "Epoch [2550/10000], loss: 0.33992 acc: 0.90667 val_loss: 0.32585, val_acc: 0.94667\n",
      "Epoch [2560/10000], loss: 0.33947 acc: 0.90667 val_loss: 0.32539, val_acc: 0.94667\n",
      "Epoch [2570/10000], loss: 0.33901 acc: 0.90667 val_loss: 0.32493, val_acc: 0.94667\n",
      "Epoch [2580/10000], loss: 0.33856 acc: 0.90667 val_loss: 0.32448, val_acc: 0.94667\n",
      "Epoch [2590/10000], loss: 0.33812 acc: 0.90667 val_loss: 0.32403, val_acc: 0.94667\n",
      "Epoch [2600/10000], loss: 0.33767 acc: 0.90667 val_loss: 0.32358, val_acc: 0.94667\n",
      "Epoch [2610/10000], loss: 0.33723 acc: 0.90667 val_loss: 0.32313, val_acc: 0.94667\n",
      "Epoch [2620/10000], loss: 0.33678 acc: 0.90667 val_loss: 0.32269, val_acc: 0.94667\n",
      "Epoch [2630/10000], loss: 0.33634 acc: 0.90667 val_loss: 0.32225, val_acc: 0.94667\n",
      "Epoch [2640/10000], loss: 0.33591 acc: 0.90667 val_loss: 0.32180, val_acc: 0.94667\n",
      "Epoch [2650/10000], loss: 0.33547 acc: 0.90667 val_loss: 0.32136, val_acc: 0.94667\n",
      "Epoch [2660/10000], loss: 0.33504 acc: 0.90667 val_loss: 0.32093, val_acc: 0.94667\n",
      "Epoch [2670/10000], loss: 0.33460 acc: 0.90667 val_loss: 0.32049, val_acc: 0.94667\n",
      "Epoch [2680/10000], loss: 0.33417 acc: 0.90667 val_loss: 0.32006, val_acc: 0.94667\n",
      "Epoch [2690/10000], loss: 0.33375 acc: 0.90667 val_loss: 0.31963, val_acc: 0.94667\n",
      "Epoch [2700/10000], loss: 0.33332 acc: 0.90667 val_loss: 0.31920, val_acc: 0.94667\n",
      "Epoch [2710/10000], loss: 0.33290 acc: 0.90667 val_loss: 0.31877, val_acc: 0.94667\n",
      "Epoch [2720/10000], loss: 0.33247 acc: 0.90667 val_loss: 0.31834, val_acc: 0.94667\n",
      "Epoch [2730/10000], loss: 0.33205 acc: 0.90667 val_loss: 0.31792, val_acc: 0.94667\n",
      "Epoch [2740/10000], loss: 0.33164 acc: 0.90667 val_loss: 0.31750, val_acc: 0.94667\n",
      "Epoch [2750/10000], loss: 0.33122 acc: 0.90667 val_loss: 0.31708, val_acc: 0.94667\n",
      "Epoch [2760/10000], loss: 0.33080 acc: 0.90667 val_loss: 0.31666, val_acc: 0.94667\n",
      "Epoch [2770/10000], loss: 0.33039 acc: 0.90667 val_loss: 0.31624, val_acc: 0.94667\n",
      "Epoch [2780/10000], loss: 0.32998 acc: 0.90667 val_loss: 0.31583, val_acc: 0.94667\n",
      "Epoch [2790/10000], loss: 0.32957 acc: 0.90667 val_loss: 0.31542, val_acc: 0.94667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2800/10000], loss: 0.32916 acc: 0.90667 val_loss: 0.31500, val_acc: 0.94667\n",
      "Epoch [2810/10000], loss: 0.32876 acc: 0.90667 val_loss: 0.31460, val_acc: 0.94667\n",
      "Epoch [2820/10000], loss: 0.32835 acc: 0.90667 val_loss: 0.31419, val_acc: 0.94667\n",
      "Epoch [2830/10000], loss: 0.32795 acc: 0.90667 val_loss: 0.31378, val_acc: 0.94667\n",
      "Epoch [2840/10000], loss: 0.32755 acc: 0.90667 val_loss: 0.31338, val_acc: 0.94667\n",
      "Epoch [2850/10000], loss: 0.32715 acc: 0.90667 val_loss: 0.31297, val_acc: 0.94667\n",
      "Epoch [2860/10000], loss: 0.32675 acc: 0.90667 val_loss: 0.31257, val_acc: 0.94667\n",
      "Epoch [2870/10000], loss: 0.32636 acc: 0.90667 val_loss: 0.31217, val_acc: 0.94667\n",
      "Epoch [2880/10000], loss: 0.32597 acc: 0.90667 val_loss: 0.31178, val_acc: 0.94667\n",
      "Epoch [2890/10000], loss: 0.32557 acc: 0.90667 val_loss: 0.31138, val_acc: 0.94667\n",
      "Epoch [2900/10000], loss: 0.32518 acc: 0.90667 val_loss: 0.31099, val_acc: 0.94667\n",
      "Epoch [2910/10000], loss: 0.32480 acc: 0.90667 val_loss: 0.31060, val_acc: 0.94667\n",
      "Epoch [2920/10000], loss: 0.32441 acc: 0.90667 val_loss: 0.31020, val_acc: 0.94667\n",
      "Epoch [2930/10000], loss: 0.32402 acc: 0.90667 val_loss: 0.30982, val_acc: 0.94667\n",
      "Epoch [2940/10000], loss: 0.32364 acc: 0.90667 val_loss: 0.30943, val_acc: 0.94667\n",
      "Epoch [2950/10000], loss: 0.32326 acc: 0.90667 val_loss: 0.30904, val_acc: 0.94667\n",
      "Epoch [2960/10000], loss: 0.32288 acc: 0.90667 val_loss: 0.30866, val_acc: 0.94667\n",
      "Epoch [2970/10000], loss: 0.32250 acc: 0.90667 val_loss: 0.30827, val_acc: 0.94667\n",
      "Epoch [2980/10000], loss: 0.32212 acc: 0.90667 val_loss: 0.30789, val_acc: 0.94667\n",
      "Epoch [2990/10000], loss: 0.32175 acc: 0.90667 val_loss: 0.30751, val_acc: 0.94667\n",
      "Epoch [3000/10000], loss: 0.32137 acc: 0.90667 val_loss: 0.30714, val_acc: 0.94667\n",
      "Epoch [3010/10000], loss: 0.32100 acc: 0.90667 val_loss: 0.30676, val_acc: 0.94667\n",
      "Epoch [3020/10000], loss: 0.32063 acc: 0.90667 val_loss: 0.30638, val_acc: 0.94667\n",
      "Epoch [3030/10000], loss: 0.32026 acc: 0.90667 val_loss: 0.30601, val_acc: 0.94667\n",
      "Epoch [3040/10000], loss: 0.31989 acc: 0.90667 val_loss: 0.30564, val_acc: 0.94667\n",
      "Epoch [3050/10000], loss: 0.31952 acc: 0.90667 val_loss: 0.30527, val_acc: 0.94667\n",
      "Epoch [3060/10000], loss: 0.31916 acc: 0.90667 val_loss: 0.30490, val_acc: 0.94667\n",
      "Epoch [3070/10000], loss: 0.31880 acc: 0.90667 val_loss: 0.30453, val_acc: 0.94667\n",
      "Epoch [3080/10000], loss: 0.31844 acc: 0.90667 val_loss: 0.30417, val_acc: 0.94667\n",
      "Epoch [3090/10000], loss: 0.31807 acc: 0.90667 val_loss: 0.30380, val_acc: 0.94667\n",
      "Epoch [3100/10000], loss: 0.31772 acc: 0.90667 val_loss: 0.30344, val_acc: 0.94667\n",
      "Epoch [3110/10000], loss: 0.31736 acc: 0.90667 val_loss: 0.30308, val_acc: 0.94667\n",
      "Epoch [3120/10000], loss: 0.31700 acc: 0.90667 val_loss: 0.30272, val_acc: 0.94667\n",
      "Epoch [3130/10000], loss: 0.31665 acc: 0.90667 val_loss: 0.30236, val_acc: 0.94667\n",
      "Epoch [3140/10000], loss: 0.31630 acc: 0.90667 val_loss: 0.30200, val_acc: 0.94667\n",
      "Epoch [3150/10000], loss: 0.31594 acc: 0.90667 val_loss: 0.30165, val_acc: 0.94667\n",
      "Epoch [3160/10000], loss: 0.31559 acc: 0.90667 val_loss: 0.30129, val_acc: 0.94667\n",
      "Epoch [3170/10000], loss: 0.31525 acc: 0.90667 val_loss: 0.30094, val_acc: 0.94667\n",
      "Epoch [3180/10000], loss: 0.31490 acc: 0.90667 val_loss: 0.30059, val_acc: 0.94667\n",
      "Epoch [3190/10000], loss: 0.31455 acc: 0.90667 val_loss: 0.30024, val_acc: 0.94667\n",
      "Epoch [3200/10000], loss: 0.31421 acc: 0.90667 val_loss: 0.29989, val_acc: 0.94667\n",
      "Epoch [3210/10000], loss: 0.31386 acc: 0.90667 val_loss: 0.29954, val_acc: 0.94667\n",
      "Epoch [3220/10000], loss: 0.31352 acc: 0.90667 val_loss: 0.29919, val_acc: 0.94667\n",
      "Epoch [3230/10000], loss: 0.31318 acc: 0.90667 val_loss: 0.29885, val_acc: 0.94667\n",
      "Epoch [3240/10000], loss: 0.31284 acc: 0.90667 val_loss: 0.29851, val_acc: 0.94667\n",
      "Epoch [3250/10000], loss: 0.31251 acc: 0.90667 val_loss: 0.29816, val_acc: 0.94667\n",
      "Epoch [3260/10000], loss: 0.31217 acc: 0.90667 val_loss: 0.29782, val_acc: 0.94667\n",
      "Epoch [3270/10000], loss: 0.31183 acc: 0.90667 val_loss: 0.29748, val_acc: 0.94667\n",
      "Epoch [3280/10000], loss: 0.31150 acc: 0.90667 val_loss: 0.29715, val_acc: 0.94667\n",
      "Epoch [3290/10000], loss: 0.31117 acc: 0.90667 val_loss: 0.29681, val_acc: 0.94667\n",
      "Epoch [3300/10000], loss: 0.31084 acc: 0.90667 val_loss: 0.29647, val_acc: 0.94667\n",
      "Epoch [3310/10000], loss: 0.31051 acc: 0.90667 val_loss: 0.29614, val_acc: 0.94667\n",
      "Epoch [3320/10000], loss: 0.31018 acc: 0.90667 val_loss: 0.29581, val_acc: 0.94667\n",
      "Epoch [3330/10000], loss: 0.30985 acc: 0.90667 val_loss: 0.29548, val_acc: 0.94667\n",
      "Epoch [3340/10000], loss: 0.30953 acc: 0.90667 val_loss: 0.29515, val_acc: 0.94667\n",
      "Epoch [3350/10000], loss: 0.30920 acc: 0.90667 val_loss: 0.29482, val_acc: 0.94667\n",
      "Epoch [3360/10000], loss: 0.30888 acc: 0.90667 val_loss: 0.29449, val_acc: 0.94667\n",
      "Epoch [3370/10000], loss: 0.30856 acc: 0.90667 val_loss: 0.29416, val_acc: 0.94667\n",
      "Epoch [3380/10000], loss: 0.30824 acc: 0.90667 val_loss: 0.29384, val_acc: 0.94667\n",
      "Epoch [3390/10000], loss: 0.30792 acc: 0.90667 val_loss: 0.29351, val_acc: 0.94667\n",
      "Epoch [3400/10000], loss: 0.30760 acc: 0.90667 val_loss: 0.29319, val_acc: 0.94667\n",
      "Epoch [3410/10000], loss: 0.30728 acc: 0.90667 val_loss: 0.29287, val_acc: 0.94667\n",
      "Epoch [3420/10000], loss: 0.30696 acc: 0.90667 val_loss: 0.29255, val_acc: 0.94667\n",
      "Epoch [3430/10000], loss: 0.30665 acc: 0.90667 val_loss: 0.29223, val_acc: 0.94667\n",
      "Epoch [3440/10000], loss: 0.30634 acc: 0.90667 val_loss: 0.29191, val_acc: 0.94667\n",
      "Epoch [3450/10000], loss: 0.30602 acc: 0.90667 val_loss: 0.29159, val_acc: 0.94667\n",
      "Epoch [3460/10000], loss: 0.30571 acc: 0.90667 val_loss: 0.29128, val_acc: 0.94667\n",
      "Epoch [3470/10000], loss: 0.30540 acc: 0.90667 val_loss: 0.29096, val_acc: 0.94667\n",
      "Epoch [3480/10000], loss: 0.30509 acc: 0.90667 val_loss: 0.29065, val_acc: 0.94667\n",
      "Epoch [3490/10000], loss: 0.30479 acc: 0.90667 val_loss: 0.29034, val_acc: 0.94667\n",
      "Epoch [3500/10000], loss: 0.30448 acc: 0.90667 val_loss: 0.29003, val_acc: 0.94667\n",
      "Epoch [3510/10000], loss: 0.30417 acc: 0.90667 val_loss: 0.28972, val_acc: 0.94667\n",
      "Epoch [3520/10000], loss: 0.30387 acc: 0.90667 val_loss: 0.28941, val_acc: 0.94667\n",
      "Epoch [3530/10000], loss: 0.30357 acc: 0.90667 val_loss: 0.28910, val_acc: 0.94667\n",
      "Epoch [3540/10000], loss: 0.30327 acc: 0.90667 val_loss: 0.28879, val_acc: 0.94667\n",
      "Epoch [3550/10000], loss: 0.30297 acc: 0.90667 val_loss: 0.28849, val_acc: 0.94667\n",
      "Epoch [3560/10000], loss: 0.30267 acc: 0.90667 val_loss: 0.28818, val_acc: 0.94667\n",
      "Epoch [3570/10000], loss: 0.30237 acc: 0.90667 val_loss: 0.28788, val_acc: 0.94667\n",
      "Epoch [3580/10000], loss: 0.30207 acc: 0.90667 val_loss: 0.28758, val_acc: 0.94667\n",
      "Epoch [3590/10000], loss: 0.30177 acc: 0.90667 val_loss: 0.28728, val_acc: 0.94667\n",
      "Epoch [3600/10000], loss: 0.30148 acc: 0.90667 val_loss: 0.28698, val_acc: 0.94667\n",
      "Epoch [3610/10000], loss: 0.30119 acc: 0.90667 val_loss: 0.28668, val_acc: 0.94667\n",
      "Epoch [3620/10000], loss: 0.30089 acc: 0.90667 val_loss: 0.28638, val_acc: 0.96000\n",
      "Epoch [3630/10000], loss: 0.30060 acc: 0.90667 val_loss: 0.28608, val_acc: 0.96000\n",
      "Epoch [3640/10000], loss: 0.30031 acc: 0.90667 val_loss: 0.28579, val_acc: 0.96000\n",
      "Epoch [3650/10000], loss: 0.30002 acc: 0.90667 val_loss: 0.28549, val_acc: 0.96000\n",
      "Epoch [3660/10000], loss: 0.29973 acc: 0.90667 val_loss: 0.28520, val_acc: 0.96000\n",
      "Epoch [3670/10000], loss: 0.29944 acc: 0.90667 val_loss: 0.28491, val_acc: 0.96000\n",
      "Epoch [3680/10000], loss: 0.29916 acc: 0.90667 val_loss: 0.28462, val_acc: 0.96000\n",
      "Epoch [3690/10000], loss: 0.29887 acc: 0.90667 val_loss: 0.28433, val_acc: 0.96000\n",
      "Epoch [3700/10000], loss: 0.29859 acc: 0.90667 val_loss: 0.28404, val_acc: 0.96000\n",
      "Epoch [3710/10000], loss: 0.29830 acc: 0.90667 val_loss: 0.28375, val_acc: 0.96000\n",
      "Epoch [3720/10000], loss: 0.29802 acc: 0.90667 val_loss: 0.28346, val_acc: 0.96000\n",
      "Epoch [3730/10000], loss: 0.29774 acc: 0.90667 val_loss: 0.28318, val_acc: 0.96000\n",
      "Epoch [3740/10000], loss: 0.29746 acc: 0.90667 val_loss: 0.28289, val_acc: 0.96000\n",
      "Epoch [3750/10000], loss: 0.29718 acc: 0.90667 val_loss: 0.28261, val_acc: 0.96000\n",
      "Epoch [3760/10000], loss: 0.29690 acc: 0.90667 val_loss: 0.28232, val_acc: 0.96000\n",
      "Epoch [3770/10000], loss: 0.29663 acc: 0.90667 val_loss: 0.28204, val_acc: 0.96000\n",
      "Epoch [3780/10000], loss: 0.29635 acc: 0.90667 val_loss: 0.28176, val_acc: 0.96000\n",
      "Epoch [3790/10000], loss: 0.29607 acc: 0.90667 val_loss: 0.28148, val_acc: 0.96000\n",
      "Epoch [3800/10000], loss: 0.29580 acc: 0.90667 val_loss: 0.28120, val_acc: 0.96000\n",
      "Epoch [3810/10000], loss: 0.29553 acc: 0.90667 val_loss: 0.28092, val_acc: 0.96000\n",
      "Epoch [3820/10000], loss: 0.29525 acc: 0.90667 val_loss: 0.28064, val_acc: 0.96000\n"
     ]
    }
   ],
   "source": [
    "# 学習率\n",
    "lr = 0.01\n",
    "\n",
    "# 初期化\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 損失関数： 交差エントロピー関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化関数: 勾配降下法\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 繰り返し回数\n",
    "num_epochs = 10000\n",
    "\n",
    "# 評価結果記録用\n",
    "history = np.zeros((0,5))\n",
    "# 繰り返し計算メインループ\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 訓練フェーズ\n",
    "    \n",
    "    #勾配の初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 予測計算\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 勾配計算\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータ修正\n",
    "    optimizer.step()\n",
    "\n",
    "    # 予測ラベル算出\n",
    "    predicted = torch.max(outputs, 1)[1]\n",
    "\n",
    "    # 損失と精度の計算\n",
    "    train_loss = loss.item()\n",
    "    train_acc = (predicted == labels).sum()  / len(labels)\n",
    "\n",
    "    #予測フェーズ\n",
    "\n",
    "    # 予測計算\n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 損失計算\n",
    "    loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "    # 予測ラベル算出\n",
    "    predicted_test = torch.max(outputs_test, 1)[1]\n",
    "\n",
    "    # 損失と精度の計算\n",
    "    val_loss =  loss_test.item()\n",
    "    val_acc =  (predicted_test == labels_test).sum() / len(labels_test)\n",
    "    \n",
    "    if ((epoch) % 10 == 0):\n",
    "        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失と精度の確認\n",
    "\n",
    "print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )\n",
    "print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (損失)\n",
    "\n",
    "plt.plot(history[:,0], history[:,1], 'b', label='訓練')\n",
    "plt.plot(history[:,0], history[:,3], 'k', label='検証')\n",
    "plt.xlabel('繰り返し回数')\n",
    "plt.ylabel('損失')\n",
    "plt.title('学習曲線(損失)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (精度)\n",
    "\n",
    "plt.plot(history[:,0], history[:,2], 'b', label='訓練')\n",
    "plt.plot(history[:,0], history[:,4], 'k', label='検証')\n",
    "plt.xlabel('繰り返し回数')\n",
    "plt.ylabel('精度')\n",
    "plt.title('学習曲線(精度)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
